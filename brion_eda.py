"""
brion_eda.py

BRION ìœ ë°©ì•” ëª¨ë¸ìš© ê³ ê¸‰ EDA ìë™ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

- `test_breast_data_varied.csv`ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •ë°€ êµ¬ì¡° ë¶„ì„
- ê²°ì¸¡ì¹˜, ë³€ìˆ˜ íƒ€ì…, ìˆ˜ì¹˜í˜•/ë²”ì£¼í˜• í†µê³„, ìƒê´€ê´€ê³„, ìœ ì‚¬ ë³€ìˆ˜ëª… íƒì§€ í¬í•¨
- ì¶œë ¥ ê²°ê³¼ëŠ” eda_structure2.txtë¡œ ì €ì¥ë˜ë©°, ì‹œê°í™” ì—†ì´ êµ¬ì¡° íŒŒì•… ì¤‘ì‹¬
- ê°œì¸ì •ë³´ ì—†ì´ ì•ˆì „í•œ ë¶„ì„ì„ ëª©í‘œë¡œ ì„¤ê³„ë¨

CSV íŒŒì¼ì€ ê°™ì€ ê²½ë¡œì— ìˆì–´ì•¼ í•˜ë©°, ì‹¤í–‰ ì‹œ eda_structure2.txt íŒŒì¼ë¡œ ì¶œë ¥ë©ë‹ˆë‹¤.
""" 

import pandas as pd
import numpy as np
from difflib import SequenceMatcher
import io
import sys
import os

# ì˜ˆì‹œì…ë‹ˆë‹¤: í•„ìš” ì‹œ ë‹¤ë¥¸ CSV íŒŒì¼ëª…ì„ ì•„ë˜ì— ì…ë ¥í•˜ì„¸ìš”.
file_path = os.path.join(os.getcwd(), "test_breast_data_varied.csv")

def full_safe_eda(df: pd.DataFrame):
    print("\U0001F4CC [1] ë°ì´í„° í¬ê¸° ë° ì»¬ëŸ¼ ìˆ˜")
    print(f"- í–‰(row) ìˆ˜: {df.shape[0]:,}")
    print(f"- ì—´(column) ìˆ˜: {df.shape[1]:,}")

    print("\n\U0001F4CC [2] ì»¬ëŸ¼ë³„ ë°ì´í„° íƒ€ì…")
    dtype_df = df.dtypes.reset_index()
    dtype_df.columns = ['ì»¬ëŸ¼ëª…', 'ë°ì´í„° íƒ€ì…']
    print(dtype_df.to_string(index=False))

    print("\n\U0001F4CC [3] ê²°ì¸¡ì¹˜ ìˆ˜ ë° ë¹„ìœ¨ (ì •ë°€) - null ë¹„ìœ¨ ì†Œìˆ˜ì  4ìë¦¬")
    null_count = df.isnull().sum()
    null_percent = (null_count / len(df) * 100).round(4)
    null_df = pd.DataFrame({'null_count': null_count, 'null_percent(%)': null_percent})
    null_df = null_df.sort_values('null_percent(%)', ascending=False)
    print(null_df)

    print("\n\U0001F4CC [4] ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ìš”ì•½ í†µê³„ + ì¶”ê°€ ì§€í‘œ")
    if not df.select_dtypes(include=[np.number]).empty:
        desc = df.describe(include=[np.number]).T
        desc['range'] = desc['max'] - desc['min']
        desc['iqr'] = desc['75%'] - desc['25%']
        desc['missing'] = df.isnull().sum()
        print(desc[['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max', 'range', 'iqr', 'missing']].round(4))
    else:
        print("â— ìˆ˜ì¹˜í˜• ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
    print("\n\U0001F4CC [5] ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„í¬ ìƒìœ„ 5ê°œ")
    cat_cols = df.select_dtypes(include='object').columns
    if len(cat_cols) == 0:
        print("â— ë²”ì£¼í˜• ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        for col in cat_cols:
            print(f"\n- {col} (ê³ ìœ ê°’ {df[col].nunique()}ê°œ / ì´ {len(df)}í–‰ ì¤‘)")
            print(df[col].value_counts(dropna=False).head(5))
            
    print("\n\U0001F4CC [6] ë¬¸ìì—´ ê¸¸ì´ í†µê³„ (min/max/mean/median/std/var)")
    for col in cat_cols:
        lengths = df[col].dropna().astype(str).apply(len)
        if not lengths.empty:
            print(f"- {col}: min={lengths.min()}, max={lengths.max()}, mean={lengths.mean():.2f}, median={lengths.median():.2f}, std={lengths.std():.2f}, var={lengths.var():.2f}")

    print("\n\U0001F4CC [7] ê° ì»¬ëŸ¼ ê³ ìœ ê°’ 5ê°œ ì˜ˆì‹œ")
    for col in df.columns:
        unique_vals = df[col].dropna().unique()
        print(f"- {col} (ê³ ìœ ê°’ {len(unique_vals)}ê°œ): {unique_vals[:5]}")

    print("\n\U0001F4CC [8] ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ (ìƒìœ„ 10ìŒ + í‰ê·  + ê°•í•œ ê´€ê³„ ë¶„ë¦¬)")
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) < 2:
        print("â— ìƒê´€ê³„ìˆ˜ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¹˜í˜• ë³€ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
    else:
        corr_matrix = df[numeric_cols].corr().round(4)
        corr_pairs = corr_matrix.unstack()
        corr_pairs = corr_pairs[corr_pairs.index.get_level_values(0) != corr_pairs.index.get_level_values(1)]
        corr_pairs = corr_pairs.dropna().sort_values(key=abs, ascending=False)
        unique_pairs = corr_pairs.groupby(lambda x: frozenset(x)).first()
        top_10 = unique_pairs.head(10)
        for (var1, var2), val in top_10.items():
            print(f"- {list(var1)[0]} â†” {list(var1)[1]}: ìƒê´€ê³„ìˆ˜ {val:.4f}")
        print(f"\n- ì „ì²´ ë³€ìˆ˜ ê°„ ìƒê´€ê³„ìˆ˜ í‰ê· : {unique_pairs.abs().mean():.4f}")
        print(f"- ì´ ë³€ìˆ˜ ìŒ ìˆ˜: {len(unique_pairs)}")
        strong_pos = unique_pairs[unique_pairs > 0.8]
        strong_neg = unique_pairs[unique_pairs < -0.8]
        print(f"- ê°•í•œ ì–‘ì˜ ìƒê´€ê´€ê³„: {len(strong_pos)}ìŒ, ê°•í•œ ìŒì˜ ìƒê´€ê´€ê³„: {len(strong_neg)}ìŒ")

    print("\n\U0001F4CC [9] ì»¬ëŸ¼ëª… íŒ¨í„´ ìë™ ë¶„ë¥˜")
    for col in df.columns:
        col_lower = col.lower()
        if 'date' in col_lower:
            print(f"[DATE] {col}")
        elif 'code' in col_lower:
            print(f"[CODE] {col}")
        elif 'flag' in col_lower or 'yn' in col_lower:
            print(f"[FLAG] {col}")

    print("\n\U0001F4CC [10] ì˜¤ì—¼ëœ íƒ€ì… íƒì§€ (ìˆ«ìí˜•ì¸ë° objectë¡œ ì €ì¥ëœ ì»¬ëŸ¼)")
    found_flag = False
    for col in cat_cols:
        try:
            df[col].astype(float)
            print(f"[ê°€ëŠ¥] '{col}' â†’ float ë³€í™˜ ê°€ëŠ¥ (ìˆ«ìí˜• ì˜¤ì—¼ ê°€ëŠ¥ì„±)")
            found_flag = True
        except:
            continue
    if not found_flag:
        print("â— ìˆ«ìí˜• ì˜¤ì—¼ëœ object ì»¬ëŸ¼ ì—†ìŒ")

    print("\n\U0001F4CC [11] í–‰ ê¸°ì¤€ ê²°ì¸¡ì¹˜ í†µê³„ + ë¶„í¬")
    df['nulls_per_row'] = df.isnull().sum(axis=1)
    print(df['nulls_per_row'].describe().round(2))
    bins = pd.cut(df['nulls_per_row'], bins=[-1, 0, 1, 3, 5, np.inf], labels=['0', '1', '2~3', '4~5', '6+'])
    print("\n- ê²°ì¸¡ì¹˜ ê°œìˆ˜ë³„ í–‰ ë¶„í¬:")
    print(bins.value_counts().sort_index())
    df.drop(columns=['nulls_per_row'], inplace=True)

    print("\n\U0001F4CC [12] ì»¬ëŸ¼ëª… ìœ ì‚¬ë„ ë¹„êµ (ëª¨ë“  ìŒ ìœ ì‚¬ë„ ì ìˆ˜ í¬í•¨, cutoff=0.75 ì´ìƒë§Œ í‘œì‹œ)")
    similarities = []
    cols = df.columns.tolist()
    for i in range(len(cols)):
        for j in range(i+1, len(cols)):
            ratio = SequenceMatcher(None, cols[i], cols[j]).ratio()
            if ratio >= 0.75:
                similarities.append((cols[i], cols[j], ratio))
    if similarities:
        similarities.sort(key=lambda x: x[2], reverse=True)
        for col1, col2, ratio in similarities:
            print(f"- '{col1}' â†” '{col2}' : ìœ ì‚¬ë„ {ratio:.4f}")
        print(f"\n- ìœ ì‚¬í•œ ì»¬ëŸ¼ ìŒ ì´ {len(similarities)}ê°œ")
        avg_ratio = np.mean([r[2] for r in similarities])
        print(f"- í‰ê·  ìœ ì‚¬ë„: {avg_ratio:.4f}")
    else:
        print("â— ìœ ì‚¬í•œ ì»¬ëŸ¼ ìŒì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    print("\n\U0001F4CC [13] ê³ ìœ ê°’ ê°œìˆ˜ ë§ì€ ë²”ì£¼í˜• ë³€ìˆ˜ (100ê°œ ì´ìƒ)")
    found_many = False
    for col in cat_cols:
        nunique = df[col].nunique()
        if nunique >= 100:
            print(f"- {col}: ê³ ìœ ê°’ {nunique}ê°œ")
            found_many = True
        else:
            print(f"- {col}: ê³ ìœ ê°’ {nunique}ê°œ (100 ë¯¸ë§Œ)")
    if not found_many:
        print("\nâœ… ì°¸ê³ : í˜„ì¬ ê³ ìœ ê°’ 100ê°œ ì´ìƒì¸ ë²”ì£¼í˜• ë³€ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤.")

    print("\nâœ… ê°œì¸ì •ë³´ ì—†ì´ ìµœëŒ€í•œì˜ êµ¬ì¡° ì •ë³´ ì •ë°€ ë¶„ì„ ì™„ë£Œ.")


buffer = io.StringIO()
sys.stdout = buffer

full_safe_eda(df)

sys.stdout = sys.__stdout__

# ì˜ˆì‹œì…ë‹ˆë‹¤: í•„ìš” ì‹œ ë‹¤ë¥¸ txt íŒŒì¼ëª…ì„ ì•„ë˜ì— ì…ë ¥í•˜ì„¸ìš”.
with open("eda_structure2.txt", "w", encoding="utf-8") as f:
    f.write(buffer.getvalue())
    
with open("eda_structure2.txt", "r", encoding="utf-8") as f:
    print(f.read())
    
print(f"\nğŸ“ ì €ì¥ ì™„ë£Œ: {os.path.abspath('eda_structure2.txt')}")
